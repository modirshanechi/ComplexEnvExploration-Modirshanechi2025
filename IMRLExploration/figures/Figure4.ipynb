{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "cd(\"../\")\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General inofrmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the fitted parameters (saved at `data/modeling/Params.CSV`) to plot the results of the model-selection in Figure 4.\n",
    "\n",
    "To re-fit the models yourself, please check the folder `src/01_ModelFitting`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Warning:*\n",
    "The results in the paper focus on the set of models with a *single* type of intrinsic rewards. The code is more general than this setting and include set of models that linearly combine different intrinsic rewards, e.g., \n",
    "$$r_{{\\rm int},t} = w_N \\, \\text{Novelty}_t + w_S \\, \\text{Surpise}_t + w_{I} \\, \\text{Inf-Gain}_t.$$\n",
    "\n",
    "The fitted parameters saved at `data/modeling/Params.CSV` include the parameters of these models as well. See the constant `model_settings` in `src/GlobalConstants.jl` for details of different models (charachterized as constraints over parameter space, e.g., by setting $w_S = w_I = 0$ for novelty-seeking).\n",
    "\n",
    "*Even when including combined models, novelty-seeking has the highest test log-likelihood, due to over-fitting and optimization challgenges of the more complex models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using PyPlot\n",
    "using IMRLExploration\n",
    "using JLD2\n",
    "using CSV\n",
    "using Random\n",
    "using Statistics\n",
    "using DataFrames\n",
    "using LogExpFunctions\n",
    "using Bootstrap\n",
    "\n",
    "using FitPopulations\n",
    "import FitPopulations: parameters, logp, sample, initialize!\n",
    "import FitPopulations: gradient_logp\n",
    "import FitPopulations: hessian_logp\n",
    "import FitPopulations: maximize_logp\n",
    "import FitPopulations: PopulationModel\n",
    "\n",
    "using ComponentArrays\n",
    "\n",
    "PyPlot.svg(true)\n",
    "rcParams = PyPlot.PyDict(PyPlot.matplotlib.\"rcParams\")\n",
    "rcParams[\"svg.fonttype\"] = \"none\"\n",
    "rcParams[\"pdf.fonttype\"] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Loading data\n",
    "# ------------------------------------------------------------------------------\n",
    "Outliers, Long_Subject, Quit_Subject, Data, Goal_type_Set, Sub_Num =\n",
    "        Read_processed_data(Plotting = false);\n",
    "Epi_len = sum(Func_EpiLenghts_all(Data)[1],dims=2)[:]\n",
    "Data_ns = Str_Input2Agents.(Data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# initializing the params\n",
    "# ------------------------------------------------------------------------------\n",
    "Param = Str_Param(; total_leak=true)\n",
    "Rs    = [1.,1.,1.]\n",
    "ws    = [1.,1.,1.]\n",
    "A = Str_Agent_Policy(Param, Rs, NIS_object(ws))\n",
    "p = parameters(A)\n",
    "p_names = [string(k) for k = keys(param2η(p))]\n",
    "m_names = [string(m) for m = keys(model_settings)]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# parameters dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "ηdf_CV = CSV.read(\"data/modeling/Params.CSV\", DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating test log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 3; CV_inds = [Func_GoalCV(i; nfolds = nfolds) for i = 1:nfolds]\n",
    "nfold_set = 1:3; \n",
    "model_set = [5,7,6,8]; # focusing only models that use single reward function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epi = 5\n",
    "# defining the array the contains the test log-p for every participant, episode, and model\n",
    "logp_vals_SbS = zeros(Sub_Num, N_epi, length(model_set))\n",
    "# defining the array the contains the test accuracy rate for every participant, episode, and model\n",
    "Acc_vals_SbS  = zeros(Sub_Num, N_epi, length(model_set))\n",
    "\n",
    "# going over all participants\n",
    "for i_sub = 1:Sub_Num\n",
    "      # going over all models\n",
    "      for i_model = eachindex(model_set)\n",
    "            # going over all folds and checking if i_sub is in the testing set of that fold\n",
    "            for i_fold = eachindex(nfold_set)\n",
    "            if i_sub ∈ CV_inds[nfold_set[i_fold]][2]\n",
    "                  # if i_sub is in the testing set of fold i_fold, then:\n",
    "                  # initialize and agent\n",
    "                  A = Str_Agent_Policy(Param, Rs, NIS_object(ws));\n",
    "                  # read-out the parameters of the corresponding model and fold\n",
    "                  η = ηdf_CV[!,m_names[model_set[i_model]] * \"_f\" * string(nfold_set[i_fold])]\n",
    "                  p = ComponentArray(parameters(A, η))\n",
    "                  # evaluate the model for every trial\n",
    "                  lps, APol = logp_pass_agent(Data_ns[i_sub], A, p);\n",
    "                  # save the log-likelihood\n",
    "                  logp_vals_SbS[i_sub,:,i_model] .= lps\n",
    "                  # evaluate and save accuracy rate\n",
    "                  ASta = [Data_ns[i_sub][j].AStates for j = 1:5];\n",
    "                  for i_epi = 1:N_epi\n",
    "                        Acc_vals_SbS[i_sub, i_epi, i_model] = \n",
    "                              mean([Func_agent_accuracy(APol[i_epi][t],ASta[i_epi][t])\n",
    "                                                for t = 1:(length(APol[i_epi])-1)])\n",
    "                  end\n",
    "                  println(\"------------------------\")\n",
    "                  @show i_sub\n",
    "                  @show m_names[model_set[i_model]]\n",
    "                  @show lps\n",
    "                  @show Acc_vals_SbS[i_sub, :, i_model]\n",
    "            end\n",
    "            end\n",
    "      end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed effect (not shown in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summing log-p over episodes\n",
    "total_logp_vals_SbS = sum(logp_vals_SbS,dims = 2)[:,1,:]\n",
    "y = sum(total_logp_vals_SbS,dims=1)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_logp_vals_SbS = sum(logp_vals_SbS,dims = 2)[:,1,:]\n",
    "Colors = [\"#004D66\",\"#0350B5\",\"#00CCF5\"]\n",
    "\n",
    "fig = figure(figsize=(8,8))\n",
    "# all subjects\n",
    "ax = subplot(2,2,1)\n",
    "y = sum(total_logp_vals_SbS,dims=1)[:]; \n",
    "y = y .- findmax(y)[1]; y = y .- log(sum(exp.(y)))\n",
    "x = 1:length(model_set); x_names = m_names[model_set]\n",
    "ax.bar(x,y, color = \"k\");\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x_names,fontsize=9)\n",
    "ax.set_ylabel(\"log P(model | Data)\")\n",
    "ax.set_title(\"all subjects\")\n",
    "\n",
    "# group-by-group\n",
    "for g = 0:2\n",
    "        ax = subplot(2,2,g + 2)\n",
    "        y = sum(total_logp_vals_SbS[Goal_type_Set .== g,:],dims=1)[:]; \n",
    "        y = y .- findmax(y)[1]; y = y .- log(sum(exp.(y)))\n",
    "        x = 1:length(model_set); x_names = m_names[model_set]\n",
    "        ax.bar(x,y, color = Colors[g + 1]);\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(x_names,fontsize=9)\n",
    "        ax.set_ylabel(\"log P(model | Data)\")\n",
    "        ax.set_title(string(g + 2) * \"CHF subjects\")\n",
    "end\n",
    "tight_layout()\n",
    "display(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random effect (Fig 4B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical inference\n",
    "subset = vcat([CV_inds[i][2] for i = nfold_set]...)\n",
    "L_names = Goal_type_Set[subset]\n",
    "\n",
    "# see src/Functions_MCMC_random_effects.jl for the details\n",
    "L_matrix = deepcopy(total_logp_vals_SbS[subset,:])\n",
    "R_matrix_samples, M_matrix_samples, R_samples_all, M_samples_all, \n",
    "      exp_r, d_exp_r, xp, pxp, exp_M, BOR = MCMC_BMS_Statistics(L_matrix,\n",
    "            N_Chains=100, N_Sampling = Int(2e5), N_Sampling_BOR = Int(2e5),\n",
    "            α = 1/size(L_matrix)[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "x_names = m_names[model_set]\n",
    "\n",
    "y = exp_r; dy = d_exp_r; x = 1:length(y)\n",
    "fig = figure(figsize=(12,6)); ax = subplot(1,2,1)\n",
    "ax.bar(x,y, color=\"k\",alpha=0.7)\n",
    "ax.plot([x[1]-1,x[end]+1],[1,1] ./ length(y), \n",
    "            linestyle=\"dashed\",linewidth=1,color=\"k\")\n",
    "title(\"Posterior Probabilities for Different Models\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x_names,fontsize=9)\n",
    "ax.set_ylabel(\"E[P(model) | Data ]\")\n",
    "ax.set_xlim([x[1]-1,x[end]+1])\n",
    "ax.set_ylim([0,0.6])\n",
    "\n",
    "y = pxp; x = 1:length(y)\n",
    "ax = subplot(1,2,2)\n",
    "ax.bar(x,y, color=\"k\")\n",
    "ax.plot([x[1]-1,x[end]+1],[1,1] ./ length(y), \n",
    "            linestyle=\"dashed\",linewidth=1,color=\"k\")\n",
    "title(\"Protected exceedence probabilities\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x_names,fontsize=9)\n",
    "ax.set_ylabel(\"P[r_m > r_m' | Data ]\")\n",
    "ax.set_xlim([x[1]-1,x[end]+1])\n",
    "ax.set_ylim([0,1.0])\n",
    "\n",
    "tight_layout()\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random effect per goal (Fig 4D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j = 0:2    \n",
    "        # all subjects\n",
    "        subset = vcat([CV_inds[i][2] for i = nfold_set]...)\n",
    "        L_names = Goal_type_Set[subset]\n",
    "        # selecting those with the goal condition j ∈ {0=2CHF, 1=3CHF, 2=4CHF}\n",
    "        subset = subset[L_names .== j]\n",
    "\n",
    "        L_matrix = deepcopy(total_logp_vals_SbS[subset,:])\n",
    "        R_matrix_samples, M_matrix_samples, R_samples_all, M_samples_all, \n",
    "                exp_r, d_exp_r, xp, pxp, exp_M, BOR = MCMC_BMS_Statistics(L_matrix,\n",
    "                N_Chains=100, N_Sampling = Int(2e5), N_Sampling_BOR = Int(2e5),\n",
    "                α = 1/size(L_matrix)[2])\n",
    "\n",
    "        x_names = m_names[model_set]\n",
    "        # average \n",
    "        y = exp_r; dy = d_exp_r; x = 1:length(y)\n",
    "        fig = figure(figsize=(12,6)); ax = subplot(1,2,1)\n",
    "        ax.bar(x,y, color=\"k\",alpha=0.7)\n",
    "        ax.plot([x[1]-1,x[end]+1],[1,1] ./ length(y), \n",
    "                linestyle=\"dashed\",linewidth=1,color=\"k\")\n",
    "        title(\"Posterior Probabilities for Different Models\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(x_names,fontsize=9)\n",
    "        ax.set_ylabel(\"E[P(model) | Data ]\")\n",
    "        ax.set_xlim([x[1]-1,x[end]+1])\n",
    "        ax.set_ylim([0,0.6])\n",
    "\n",
    "        y = pxp; x = 1:length(y)\n",
    "        ax = subplot(1,2,2)\n",
    "        ax.bar(x,y, color=\"k\")\n",
    "        ax.plot([x[1]-1,x[end]+1],[1,1] ./ length(y), \n",
    "                linestyle=\"dashed\",linewidth=1,color=\"k\")\n",
    "        title(\"Protected exceedence probabilities\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(x_names,fontsize=9)\n",
    "        ax.set_ylabel(\"P[r_m > r_m' | Data ]\")\n",
    "        ax.set_xlim([x[1]-1,x[end]+1])\n",
    "        ax.set_ylim([0,1.0])\n",
    "\n",
    "        tight_layout()\n",
    "        display(fig)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy rate for novelty-seeking (Fig 4C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nov_Acc_vals_SbS = Acc_vals_SbS[:,:,m_names[model_set] .== \"N\"][:,:,1]\n",
    "\n",
    "fig = figure(figsize=(6,8))\n",
    "# all subjects\n",
    "ax = subplot(2,2,1)\n",
    "y = mean(Nov_Acc_vals_SbS,dims=1)[:]; \n",
    "dy = std(Nov_Acc_vals_SbS,dims=1)[:] ./ sqrt(size(Nov_Acc_vals_SbS)[1]); \n",
    "x = 1:length(y); x_names = [\"Epi \" * string(i) for i = eachindex(y)]\n",
    "ax.bar(x,y, color = \"k\", alpha = 0.7);\n",
    "ax.errorbar(x,y[:],yerr=dy[:],color=\"k\",\n",
    "            linewidth=1,drawstyle=\"steps\",linestyle=\"\",capsize=3)\n",
    "ax.plot([x[1] - 1, x[end] + 1],[1,1] ./ 3, \"--k\");\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x_names,fontsize=9)\n",
    "ax.set_ylabel(\"average accuracy\")\n",
    "ax.set_title(\"all subjects\")\n",
    "ax.set_ylim([1/3,1]); ax.set_xlim([x[1] - 1, x[end] + 1])\n",
    "\n",
    "# group-by-group\n",
    "for g = 0:2\n",
    "      ax = subplot(2,2,g + 2)\n",
    "      y = mean(Nov_Acc_vals_SbS[Goal_type_Set .== g,:],dims=1)[:]; \n",
    "      dy = std(Nov_Acc_vals_SbS,dims=1)[:] ./ sqrt(sum(Goal_type_Set .== g)); \n",
    "      x = 1:length(y); x_names = [\"Epi \" * string(i) for i = eachindex(y)]\n",
    "      ax.bar(x,y, color = Colors[g + 1]);\n",
    "      ax.errorbar(x,y[:],yerr=dy[:],color=\"k\",\n",
    "            linewidth=1,drawstyle=\"steps\",linestyle=\"\",capsize=3)\n",
    "      ax.plot([x[1] - 1, x[end] + 1],[1,1] ./ 3, \"--k\");\n",
    "      ax.set_xticks(x)\n",
    "      ax.set_xticklabels(x_names,fontsize=9)\n",
    "      ax.set_ylabel(\"average accuracy\")\n",
    "      ax.set_title(string(g + 2) * \"CHF subjects\")\n",
    "      ax.set_ylim([1/3,1]); ax.set_xlim([x[1] - 1, x[end] + 1])\n",
    "end\n",
    "tight_layout()\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

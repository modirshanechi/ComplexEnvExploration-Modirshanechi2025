{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1. General inofrmation](#1.-general-inofrmation)\n",
    "- [2. Loading and working with the raw data](#2.-loading-and-working-with-the-raw-data)\n",
    "- [3. Loading and working with the processed data](#3.-loading-and-working-with-the-processed-data)\n",
    "- [4. Simulating the models](#4.-simulating-the-models)\n",
    "- [5. Evaluating log-likelihood](#5.-evaluating-log-likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General inofrmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using PyPlot\n",
    "using IMRLExploration\n",
    "using DataFrames\n",
    "using Statistics\n",
    "using HypothesisTests\n",
    "\n",
    "PyPlot.svg(true)\n",
    "rcParams = PyPlot.PyDict(PyPlot.matplotlib.\"rcParams\")\n",
    "rcParams[\"svg.fonttype\"] = \"none\"\n",
    "rcParams[\"pdf.fonttype\"] = 42\n",
    "\n",
    "Colors = [\"#004D66\",\"#0350B5\",\"#00CCF5\"]\n",
    "Legends = [\"CHF2\",\"CHF3\",\"CHF4\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading and working with the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you would like to only use the data without using our code, you can find a minimal version of the raw data saved in `data/tidydata.CSV` (with the same notation as in the paper).**\n",
    "\n",
    "However, if you'd like to use code, this is how we can read the data of all 63 participants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Read_data_all();\n",
    "length(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Data[n]` corresponds to the data of participant `n` in the format of structure `Str_Input` (see `src/Structs_form.jl`).\n",
    "\n",
    "`Str_Input` has the following entries:\n",
    "\n",
    "- `Sub`: Subject id (between 1 to 63)\n",
    "\n",
    "- `Gender`: 1 for male and 0 for female\n",
    "\n",
    "- `actions`: `actions[i]` is the sequence of actions during episode `i` (each action denoted by 0, 1, or 2)\n",
    "\n",
    "- `states`: `states[i]` is a coarse grained sequence of states during episode `i` with all stochastic states put together as state `7` and all goal states put together as state `0`; trap state 7 and 8 are encoded as states `8` and `9`, respectively\n",
    "\n",
    "- `images`: `images[i][:,t]` is a vector corresponding to the exact state at time step `t` during episode `i`. `images[i][1,t]` denotes whether the state is in the stochastic part (if equal to `1`), is a goal state (if equal to `2`) or is a normal state (if equal to `0`). For the normal states (states 1 to 8), `images[i][2,t]` corresponds to `states[i][t]`. For the goal state, `images[i][2,t]` denotes the value of the goal (`0` for 2CHF, `1` for 3CHF, and `2` for 4CHF). For the stochastic states, `images[i][2,t]` corresponds to the index of the stochastic state (between 0 to 49).\n",
    "\n",
    "- `trial_time`: `trial_time[i]` is the sequence of real times of the start of each trial, during episode `i`\n",
    "\n",
    "- `resp_time`: `resp_time[i]` is the sequence of reaction times during episode `i`\n",
    "\n",
    "- `TM`: the transition matrix for this participant. It is the same for all participants except for state 4, because of the action manipulation described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the data of participant 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sub = 3\n",
    "d = Data[n_sub];\n",
    "@show fieldnames(typeof(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.images[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the data in episode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epi = 1;\n",
    "T = length(d.states[n_epi])\n",
    "\n",
    "fig = figure(figsize=(12,6));\n",
    "ax = subplot(2,2,1)\n",
    "ax.plot(1:T, d.states[n_epi],\"-o\",color=\"k\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 7], d.states[n_epi][d.states[n_epi] .== 7],\"o\",color=\"b\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 8], d.states[n_epi][d.states[n_epi] .== 8],\"o\",color=\"r\")\n",
    "ax.legend([\"all states\",\"stochastic states\", \"state 8 (7 in the paper; trap)\"])\n",
    "ax.set_xlabel(\"t\"); ax.set_ylabel(\"states[n_epi]\"); \n",
    "ax.set_xlim([0,T+1]); ax.set_ylim([-0.1,9.1]); \n",
    "\n",
    "ax = subplot(2,2,3)\n",
    "ax.plot(1:T, d.actions[n_epi],\"-o\",color=\"k\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 7], d.actions[n_epi][d.states[n_epi] .== 7],\"o\",color=\"b\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 8], d.actions[n_epi][d.states[n_epi] .== 8],\"o\",color=\"r\")\n",
    "ax.legend([\"all states\",\"stochastic states\", \"state 8 (7 in the paper; trap)\"])\n",
    "ax.set_xlabel(\"t\"); ax.set_ylabel(\"actions[n_epi]\"); \n",
    "ax.set_xlim([0,T+1]); ax.set_ylim([-0.1,2.1]); \n",
    "\n",
    "ax = subplot(1,2,2)\n",
    "ax.plot(d.images[n_epi][1,:] .+ (rand(T) .* 0.2) , d.images[n_epi][2,:], \"o\", color = \"k\", alpha = 0.5)\n",
    "ax.set_xlabel(\"images[n_epi][1,:]\"); ax.set_ylabel(\"images[n_epi][2,:]\"); \n",
    "\n",
    "tight_layout()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading and working with the processed data\n",
    "\n",
    "You can directly read the processed data (after removing the outliers) by using `Read_processed_data`\n",
    "\n",
    "The function returns the following variables:\n",
    "\n",
    "- `Outliers`: The vector of lenght 63 indicating outliers (= `Long_Subject .| Quit_Subject`).\n",
    "\n",
    "- `Long_Subject`: The vector of lenght 63 indicating the subjects who had more than 3 times group averaged actions (see the paper).\n",
    "\n",
    "- `Quit_Subject`: The vector of lenght 63 indicating the subjects who had quit the experiments.\n",
    "\n",
    "- `Data`: The vector of Str_Input (the lenght must be 57).\n",
    "\n",
    "- `Goal_type_Set`: The vector of goal types (0=2CHF, 1=3CHF, 2=4CHF; the lenght must be 57).\n",
    "\n",
    "- `Sub_Num`: Number of subjects (must be 57)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Loading data\n",
    "# ------------------------------------------------------------------------------\n",
    "Outliers, Long_Subject, Quit_Subject, Data, Goal_type_Set, Sub_Num =\n",
    "        Read_processed_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replicate Fig 2A (simplification of `Func_plot_state_ratio_Epi1`; see `figures/Figure2AC.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing episode 1\n",
    "Epi = 1;\n",
    "# information of states\n",
    "Traps = [8,9]; Stoch = [7];\n",
    "All_states = [1,2,3,4,5,6,7,8,9];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we use the function `Func_desired_states_visit` which extracts the information of vising a set of `DesiredStates` for all participants. The idea is to extract how many time a participant visits `DesiredStates` and, during each visit, how long he/she stays in the `DesiredStates` (similar in to Fig. 2A in [Xu and Modirshanechi et al. 2021 in PLOS Comp. Bio.](https://doi.org/10.1371/journal.pcbi.1009070)).\n",
    "\n",
    "To test, for example, let's put `DesiredStates = Traps` or `DesiredStates = Stoch`, and let's, for now, not specify to split the episode into two parts, i.e., `first_half = false, second_half = false`. Then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, dy, N, Lenghts, y_med, y_Q25, y_Q75 = \n",
    "        Func_desired_states_visit(Data, DesiredStates = Traps,\n",
    "                                    Epi= Epi,\n",
    "                                    first_half = false,\n",
    "                                    second_half = false);\n",
    "fig = figure(figsize = (8,4))\n",
    "ax = subplot(1,1,1)\n",
    "for i_sub = 1:Sub_Num\n",
    "    ax.plot(Lenghts[i_sub])\n",
    "end\n",
    "ax.set_xlabel(\"Number of visits of the trap states\")\n",
    "ax.set_ylabel(\"Number of actions during each visit\")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, dy, N, Lenghts, y_med, y_Q25, y_Q75 = \n",
    "        Func_desired_states_visit(Data, DesiredStates = Stoch,\n",
    "                                    Epi= Epi,\n",
    "                                    first_half = false,\n",
    "                                    second_half = false);\n",
    "fig = figure(figsize = (8,4))\n",
    "ax = subplot(1,1,1)\n",
    "for i_sub = 1:Sub_Num\n",
    "    ax.plot(Lenghts[i_sub])\n",
    "end\n",
    "ax.set_xlabel(\"Number of visits of the stochastic part\")\n",
    "ax.set_ylabel(\"Number of actions during each visit\")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can evaluate the total number of actions that each participant takes within the first and the second halves of Episode 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTraps = zeros(Sub_Num,2)\n",
    "NStoch = zeros(Sub_Num,2)\n",
    "NAll   = zeros(Sub_Num,2)\n",
    "\n",
    "Options = [[ true, false],      # first half\n",
    "            [false, true ]]     # second half\n",
    "for i_opt = 1:2\n",
    "        # extracting the information for the traps\n",
    "        y, dy, N, Lenghts, y_med, y_Q25, y_Q75 = \n",
    "                Func_desired_states_visit(Data, DesiredStates = Traps,\n",
    "                                    Epi= Epi,\n",
    "                                    first_half = Options[i_opt][1],\n",
    "                                    second_half = Options[i_opt][2])\n",
    "        NTraps[:,i_opt] = sum.(Lenghts)\n",
    "        # extracting the information for the stochastic part\n",
    "        y, dy, N, Lenghts, y_med, y_Q25, y_Q75 =  \n",
    "                Func_desired_states_visit(Data, DesiredStates = Stoch,\n",
    "                                    Epi= Epi,\n",
    "                                    first_half = Options[i_opt][1],\n",
    "                                    second_half = Options[i_opt][2])\n",
    "        NStoch[:,i_opt] = sum.(Lenghts)\n",
    "        # extracting the information for \"all states\" = the lenght of the 1st/2nd halves\n",
    "        y, dy, N, Lenghts, y_med, y_Q25, y_Q75 =  \n",
    "                Func_desired_states_visit(Data, DesiredStates = All_states,\n",
    "                                    Epi= Epi,\n",
    "                                    first_half = Options[i_opt][1],\n",
    "                                    second_half = Options[i_opt][2])\n",
    "        NAll[:,i_opt] = sum.(Lenghts)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a participant-by-participant version (equivalent to the data points in Fig 2A):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize = (12,10))\n",
    "ax = subplot(2,1,1)\n",
    "ax.bar((0:3:((Sub_Num-1)*3)) .+ 0, NTraps[:,1] ./ NAll[:,1], color=\"r\")\n",
    "ax.bar((0:3:((Sub_Num-1)*3)) .+ 1, NStoch[:,1] ./ NAll[:,1], color=\"b\")\n",
    "ax.set_xticks((0:9:((Sub_Num-1)*3)) .+ 0.5); ax.set_xticklabels(1:3:Sub_Num)\n",
    "ax.set_xlim([-1,3*Sub_Num -1]); ax.set_ylim([0,1])\n",
    "ax.legend([\"traps\", \"stoch part\"])\n",
    "ax.set_xlabel(\"Participant\")\n",
    "ax.set_ylabel(\"Fraction of time steps, during the 1st half of E1, in ...\")\n",
    "ax = subplot(2,1,2)\n",
    "ax.bar((0:3:((Sub_Num-1)*3)) .+ 0, NTraps[:,2] ./ NAll[:,2], color=\"r\")\n",
    "ax.bar((0:3:((Sub_Num-1)*3)) .+ 1, NStoch[:,2] ./ NAll[:,2], color=\"b\")\n",
    "ax.set_xticks((0:9:((Sub_Num-1)*3)) .+ 0.5); ax.set_xticklabels(1:3:Sub_Num)\n",
    "ax.set_xlim([-1,3*Sub_Num -1]); ax.set_ylim([0,1])\n",
    "ax.legend([\"traps\", \"stoch part\"])\n",
    "ax.set_xlabel(\"Participant\")\n",
    "ax.set_ylabel(\"Fraction of time steps, during the 2nd half of E1, in ...\")\n",
    "\n",
    "display(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaged version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_traps = NTraps ./ NAll\n",
    "Y_stoch = NStoch ./ NAll\n",
    "Y = cat(Y_traps,Y_stoch,dims = 3)\n",
    "\n",
    "mY = mean(Y,dims=1)[1,:,:]\n",
    "dY = std(Y,dims=1)[1,:,:] ./ sqrt(Sub_Num)\n",
    "\n",
    "# X-axis information\n",
    "x_0 = [1,4]; x_0ticks = [\"Traps\", \"Stoch\"]; σ = 0.2\n",
    "\n",
    "fig = figure(figsize=(4,7)); ax = subplot(1,1,1)\n",
    "for i=1:2\n",
    "        x = x_0 .+ (i-1)\n",
    "        ax.bar(x,mY[i,:], color = Colors[i])\n",
    "end\n",
    "ax.legend([\"1st half\", \"2nd half\"])\n",
    "for i=1:2\n",
    "        x = x_0 .+ (i-1)\n",
    "        ax.errorbar(x,mY[i,:],yerr=dY[i,:],color=\"k\",\n",
    "                    linewidth=1,drawstyle=\"steps\",linestyle=\"\",capsize=3)\n",
    "        for j = 1:Sub_Num\n",
    "                x_plot = x .+ 2*σ*(rand() - 0.5)\n",
    "                ax.plot(x_plot,Y[j,i,:],\".k\",alpha = 0.5)\n",
    "        end\n",
    "end\n",
    "ax.set_xticks(x_0.+0.5)\n",
    "ax.set_xticklabels(x_0ticks)\n",
    "ax.set_title(\"Fraction of time in Epi 1\")\n",
    "ax.set_xlim([0,6])\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Simulating the models\n",
    "\n",
    "For modeling part, we need some additional packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "using Random\n",
    "using LogExpFunctions\n",
    "using FitPopulations    # = an older version of LaplacianExpectationMaximization.jl\n",
    "using ComponentArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's play around a bit with the computational models and see how we can simulate them. Let's first see how we can define an \"agent.\" \n",
    "\n",
    "The general idea is that an agent has two parts: The \"policy\" part that includes the internal variables such as Q-values and action policy (specified by `Str_Agent_Policy`) and the \"state\" part that includes the agent's action and environmental state (specified by `Str_Agent_State`). \n",
    "\n",
    "The logic is that `Str_Agent_State` can be evaluated purely based on the data, with no modeling assumption, while `Str_Agent_Policy` captures all the modeling variables. \n",
    "In other words, `Str_Agent_State` can be seen as an alternative way to represent the participants' data; see `src\\Structs_form.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the agent type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to initialize the \"type\" of model we want to work with, by initializing `Str_Agent_Policy`. At this point, we only need to care about 3 things:\n",
    "\n",
    "1. `total_leak` variable: Indicating whether the total count of states $\\tilde{C}_s^{(t)}$ used for novelty will be leaked; it is always set to `true` for all algorithms in the paper.\n",
    "\n",
    "2. `back_leak` variable: Indicator of whether there is a background leak of all counts; it is only set to `true` for the control algorithms in Section 3.4 of SI.\n",
    "\n",
    "3. Combined intrinsic rewards using `NIS_object`: While the results in the paper focus on the set of models with a *single* type of intrinsic rewards, the implementation here is is more general and allows linearly combining different intrinsic rewards, e.g., \n",
    "$$r_{{\\rm int},t} = w_N \\, \\text{Novelty}_t + w_{I} \\, \\text{Inf-Gain}_t + w_S \\, \\text{Surpise}_t.$$\n",
    "The `NIS_object` makes this possible efficiently through appropriate memory allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying total_leak and back_leak\n",
    "Param = Str_Param(total_leak = true, back_leak = false); \n",
    "# for now, you can see these as just some dummy variables\n",
    "Rs = [1.,1.,1.]; ws = [1.,1.,1.];\n",
    "# initializaing the agent\n",
    "A = Str_Agent_Policy(Param, Rs, NIS_object(ws))\n",
    "for name in fieldnames(Str_Agent_Policy)\n",
    "    println(\"$(name)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is the detailed explanation of the fields of `Str_Agent_Policy`:\n",
    "| Field of `Str_Agent_Policy` | Meaning|\n",
    "|-----------------------|----------------------------------------|\n",
    "| `Param`               | The set of parameters used by the agent (see below)|\n",
    "| `Func_eR_sas`         | A function that receives $s$, $a$, and $s'$ and returns the extrinsic rewards; see `src/Functions_for_environments.jl`|\n",
    "| `Func_iR_sas`         | A function that receives $s$, $a$, and $s'$ and returns the intrinsic rewards; see `src/Functions_for_rewards.jl`|\n",
    "| `C_s`                 | $\\tilde{C}_s^{(t)}$ in the paper|\n",
    "| `C_sa`                | $\\tilde{C}_{s,a}^{(t)}$ in the paper|\n",
    "| `C_sas`               | $\\tilde{C}_{s,a,s'}^{(t)}$ in the paper|\n",
    "| `θ_sas`               | $p^{(t)}(s';s,a) = \\hat{\\theta}^{(t)}_{s,a,s'}$ in the paper|\n",
    "| `eR_sas`              | The array of extrinsic rewards evaluated using `Func_eR_sas`|\n",
    "| `iR_sas`              | The array of intrinsic rewards evaluated using `Func_iR_sas`|\n",
    "| `Q_MBe`               | $Q_{\\rm MB, ext}^{(t)}$ in the paper|\n",
    "| `Q_MBi`               | $Q_{\\rm MB, int}^{(t)}$ in the paper|\n",
    "| `U_e`                 | $U_{\\rm MB, ext}^{(t)}$ in the paper (for prioritized sweeping)|\n",
    "| `U_i`                 | $U_{\\rm MB, int}^{(t)}$ in the paper (for prioritized sweeping)|\n",
    "| `V_dummy`             | Some dummy vector defined for *a priori* memory allocation needed for prioritized sweeping (necessary for the automatic differentiation)|\n",
    "| `P_dummy`             | Some dummy vector defined for *a priori* memory allocation needed for prioritized sweeping (necessary for the automatic differentiation)|\n",
    "| `Q_MFe`               | $Q_{\\rm MF, ext}^{(t)}$ in the paper|\n",
    "| `Q_MFi`               | $Q_{\\rm MF, int}^{(t)}$ in the paper|\n",
    "| `E_e`                 | $e_{\\rm ext}^{(t)}$ in the paper|\n",
    "| `E_i`                 | $e_{\\rm int}^{(t)}$ in the paper|\n",
    "| `Q_MB_t`              | weighted combination of the MB Q-values at time $t$|\n",
    "| `Q_MF_t`              | weighted combination of the MF Q-values at time $t$|\n",
    "| `Q_t`                 | weighted combination of all Q-values at time $t$|\n",
    "| `π_A_t`               | $\\pi_{t}$ in the paper (i.e., the softmax transformation of `Q_t`)|\n",
    "| `eR_t`                | $r_{{\\rm ext}, t}$ in the paper|\n",
    "| `eRPE_t`              | $RPE_{{\\rm ext}, t}$ in the paper||\n",
    "| `iR_t`                | $r_{{\\rm int}, t}$ in the paper|\n",
    "| `iRPE_t`              | $RPE_{{\\rm int}, t}$ in the paper|\n",
    "| `eR_max_t`            | Maximum extrinsic reward found so far (indicating the degree of reward optimism)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent's parameters\n",
    "\n",
    "Now that we have initialized the agent `A`, we need to specify the set of parameters for this agent. This can be done in an outer loop to facilate model-fitting and simulations. To do so, we can use the function `parameters` from the `FitPopulations.jl` package (an older version of `LaplacianExpectationMaximization.jl`) to define the set a set of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = parameters(A);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However, `p` is now in the space of real numbers, for the sake of optimization (see the paper's SI for details). We can convert it to using `param2η`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "η = param2η(p)\n",
    "η_names = [string(k) for k = keys(η)]\n",
    "for (k, v) in pairs(η)\n",
    "    println(\"$(k) = $(v)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are slight differences in the notations between the code and the paper; here is the detailed explanation of the fields of `η`:\n",
    "| Field of `η` | Meaning|\n",
    "|----------|----------|\n",
    "| `κ`           | $\\kappa$ in the paper|\n",
    "| `ϵ_new`       | $\\epsilon_{\\rm new}$ in the paper|\n",
    "| `ϵ_obs`       | $\\epsilon_{\\rm known}$ in the paper|\n",
    "| `λ_e`       | $\\gamma_{\\rm ext}$ in the paper|\n",
    "| `λ_i`       | $\\gamma_{\\rm int}$ in the paper|\n",
    "| `T_PS_e`       | $T_{PS,{\\rm ext}}$ in the paper|\n",
    "| `T_PS_i`       | $T_{PS,{\\rm int}}$ in the paper|\n",
    "| `ρ`       | $\\rho$ in the paper|\n",
    "| `μ_e`       | $\\lambda_{\\rm ext}$ in the paper|\n",
    "| `μ_i`       | $\\lambda_{\\rm int}$ in the paper|\n",
    "| `Q_e0`       | $Q_{\\rm MF, ext}^{(0)}$ in the paper|\n",
    "| `Q_i0`       | $Q_{\\rm MF, int}^{(0)}$ in the paper|\n",
    "| `β_MBe_1`     | $\\beta_{\\rm MB, ext}^{(1)}$ in the paper|\n",
    "| `β_MBe_2_G0`     | $\\beta_{\\rm MB, ext}^{(2, {\\rm 2CHF})}$ in the paper|\n",
    "| `β_MBe_2_G1`     | $\\beta_{\\rm MB, ext}^{(2, {\\rm 3CHF})}$ in the paper|\n",
    "| `β_MBe_2_G2`     | $\\beta_{\\rm MB, ext}^{(2, {\\rm 4CHF})}$ in the paper|\n",
    "| `β_MBi_1`     | $\\beta_{\\rm MB, int}^{(1)}$ in the paper|\n",
    "| `β_MBi_2_G0`     | $\\beta_{\\rm MB, int}^{(2, {\\rm 2CHF})}$ in the paper|\n",
    "| `β_MBi_2_G1`     | $\\beta_{\\rm MB, int}^{(2, {\\rm 3CHF})}$ in the paper|\n",
    "| `β_MBi_2_G2`     | $\\beta_{\\rm MB, int}^{(2, {\\rm 4CHF})}$ in the paper|\n",
    "| `β_MFe_1`     | $\\beta_{\\rm MF, ext}^{(1)}$ in the paper|\n",
    "| `β_MFe_2_G0`     | $\\beta_{\\rm MF, ext}^{(2, {\\rm 2CHF})}$ in the paper|\n",
    "| `β_MFe_2_G1`     | $\\beta_{\\rm MF, ext}^{(2, {\\rm 3CHF})}$ in the paper|\n",
    "| `β_MFe_2_G2`     | $\\beta_{\\rm MF, ext}^{(2, {\\rm 4CHF})}$ in the paper|\n",
    "| `β_MFi_1`     | $\\beta_{\\rm MF, int}^{(1)}$ in the paper|\n",
    "| `β_MFi_2_G0`     | $\\beta_{\\rm MF, int}^{(2, {\\rm 2CHF})}$ in the paper|\n",
    "| `β_MFi_2_G1`     | $\\beta_{\\rm MF, int}^{(2, {\\rm 3CHF})}$ in the paper|\n",
    "| `β_MFi_2_G2`     | $\\beta_{\\rm MF, int}^{(2, {\\rm 4CHF})}$ in the paper|\n",
    "| `Q_bias_1`     | $b({\\rm middle})$ in the paper|\n",
    "| `Q_bias_2`     | $b({\\rm right})$ in the paper|\n",
    "| `r_1`     | $r_1^*$ in the paper|\n",
    "| `r_2`     | $r_2^*$ in the paper|\n",
    "| `wN`     | $w_N$ above|\n",
    "| `wI`     | $w_I$ above|\n",
    "| `wS`     | $w_S$ above|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set the variables as we wish (or, e.g., using `IMRLExploration.η0_efficient_sim` for efficient simulations) by manipulating `η`.\n",
    "\n",
    "WARNING: Note that changing `T_PS_e` and `T_PS_i` will be over-written by the system to always keep them at 100 (see the paper's SI). Similarly, changing `β_MBe_1` will be overwritten by the system to always keep it at 0.1; this is done without the loss of generality as `wN`, `wI`, and `wS` are not bounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider perfect model-building\n",
    "η = merge(η, (;κ = 1.))\n",
    "# let's remove the model-free branch\n",
    "η = merge(η, (;β_MFe_1 = 0., β_MFe_2_G0 = 0., β_MFe_2_G1 = 0., β_MFe_2_G2 = 0.))\n",
    "η = merge(η, (;β_MFi_1 = 0., β_MFi_2_G0 = 0., β_MFi_2_G1 = 0., β_MFi_2_G2 = 0.))\n",
    "# let's assume there is no difference between the extrinsic reward-values of different goals\n",
    "η = merge(η, (;r_1 = 1., r_2 = 1.))\n",
    "# and let's consider novlety-seeking\n",
    "η = merge(η, (;wN = 30., wI = 0., wS = 0.))\n",
    "\n",
    "# this can now be transformed back to the real-value space:\n",
    "p = parameters(A,η);\n",
    "\n",
    "for (k, v) in pairs(η)\n",
    "    println(\"$(k) = $(v)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "Now that we have initialzed the agent type `A` and have specified its parameters `p`, we can go on and simulate the agent for 5 episodes in our experimental paradigm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the random seed\n",
    "rng = MersenneTwister(2023)\n",
    "# setting the goal parobability\n",
    "G_type_prob = [1.,0.,0.]    # i.e., 100% probability of being in an Environment with the 2CHF goal\n",
    "# simulation\n",
    "simulation_results = simulate(A, p; G_type_prob = G_type_prob, ifpass_env = true, rng = rng);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`simulation_results` has three parts:\n",
    "\n",
    "1. `simulation_results.data` has 5 elements corresponding to 5 simulated episodes. Each element contains two parts:\n",
    "\n",
    "    1.1. `simulation_results.data[i].AStates` is the sequence of `Str_Agent_State` structs druing episode `i`.\n",
    "    \n",
    "    1.2. `simulation_results.data[i].G_type` is the goal type of episode `i` (must be the same for all episodes).\n",
    "\n",
    "2. `simulation_results.logp` is the log-likelihood of the simulated data under the true parameters.\n",
    "\n",
    "3. `simulation_results.TM` is the transition matrix of the environment (after the action manipulation in state 4; see the paper).\n",
    "\n",
    "We can transform `simulation_results` into the data type of `Str_Input` (just like the participants' data analyzed above) and analyze it similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Str_SASeq2Input(simulation_results);\n",
    "n_epi = 1;\n",
    "T = length(d.states[n_epi])\n",
    "\n",
    "fig = figure(figsize=(12,6));\n",
    "ax = subplot(2,2,1)\n",
    "ax.plot(1:T, d.states[n_epi],\"-o\",color=\"k\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 7], d.states[n_epi][d.states[n_epi] .== 7],\"o\",color=\"b\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 8], d.states[n_epi][d.states[n_epi] .== 8],\"o\",color=\"r\")\n",
    "ax.legend([\"all states\",\"stochastic states\", \"state 8 (7 in the paper; trap)\"])\n",
    "ax.set_xlabel(\"t\"); ax.set_ylabel(\"states[n_epi]\"); \n",
    "ax.set_xlim([0,T+1]); ax.set_ylim([-0.1,9.1]); \n",
    "\n",
    "ax = subplot(2,2,3)\n",
    "ax.plot(1:T, d.actions[n_epi],\"-o\",color=\"k\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 7], d.actions[n_epi][d.states[n_epi] .== 7],\"o\",color=\"b\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 8], d.actions[n_epi][d.states[n_epi] .== 8],\"o\",color=\"r\")\n",
    "ax.legend([\"all states\",\"stochastic states\", \"state 8 (7 in the paper; trap)\"])\n",
    "ax.set_xlabel(\"t\"); ax.set_ylabel(\"actions[n_epi]\"); \n",
    "ax.set_xlim([0,T+1]); ax.set_ylim([-0.1,2.1]); \n",
    "\n",
    "ax = subplot(1,2,2)\n",
    "ax.plot(d.images[n_epi][1,:] .+ (rand(T) .* 0.2) , d.images[n_epi][2,:], \"o\", color = \"k\", alpha = 0.5)\n",
    "ax.set_xlabel(\"images[n_epi][1,:]\"); ax.set_ylabel(\"images[n_epi][2,:]\"); \n",
    "\n",
    "tight_layout()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate a surprise-seeking agent now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "η = merge(η, (;wN = 0., wI = 0., wS = 30.))\n",
    "p = parameters(A,η);\n",
    "rng = MersenneTwister(2023)\n",
    "G_type_prob = [1.,0.,0.]\n",
    "simulation_results = simulate(A, p; G_type_prob = G_type_prob, ifpass_env = true, rng = rng);\n",
    "d = Str_SASeq2Input(simulation_results);\n",
    "\n",
    "n_epi = 1;\n",
    "T = length(d.states[n_epi])\n",
    "\n",
    "fig = figure(figsize=(12,6));\n",
    "ax = subplot(2,2,1)\n",
    "ax.plot(1:T, d.states[n_epi],\"-o\",color=\"k\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 7], d.states[n_epi][d.states[n_epi] .== 7],\"o\",color=\"b\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 8], d.states[n_epi][d.states[n_epi] .== 8],\"o\",color=\"r\")\n",
    "ax.legend([\"all states\",\"stochastic states\", \"state 8 (7 in the paper; trap)\"])\n",
    "ax.set_xlabel(\"t\"); ax.set_ylabel(\"states[n_epi]\"); \n",
    "ax.set_xlim([0,T+1]); ax.set_ylim([-0.1,9.1]); \n",
    "\n",
    "ax = subplot(2,2,3)\n",
    "ax.plot(1:T, d.actions[n_epi],\"-o\",color=\"k\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 7], d.actions[n_epi][d.states[n_epi] .== 7],\"o\",color=\"b\")\n",
    "ax.plot((1:T)[d.states[n_epi] .== 8], d.actions[n_epi][d.states[n_epi] .== 8],\"o\",color=\"r\")\n",
    "ax.legend([\"all states\",\"stochastic states\", \"state 8 (7 in the paper; trap)\"])\n",
    "ax.set_xlabel(\"t\"); ax.set_ylabel(\"actions[n_epi]\"); \n",
    "ax.set_xlim([0,T+1]); ax.set_ylim([-0.1,2.1]); \n",
    "\n",
    "ax = subplot(1,2,2)\n",
    "ax.plot(d.images[n_epi][1,:] .+ (rand(T) .* 0.2) , d.images[n_epi][2,:], \"o\", color = \"k\", alpha = 0.5)\n",
    "ax.set_xlabel(\"images[n_epi][1,:]\"); ax.set_ylabel(\"images[n_epi][2,:]\"); \n",
    "\n",
    "tight_layout()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluating log-likelihood\n",
    "\n",
    "We can use the same setting as described above for model-based study of participants' behavior.\n",
    "\n",
    "To do so, we can first specify the model similarly to before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializaing the agent\n",
    "Param = Str_Param(total_leak = true, back_leak = false); \n",
    "Rs = [1.,1.,1.]; ws = [1.,1.,1.];\n",
    "A = Str_Agent_Policy(Param, Rs, NIS_object(ws))\n",
    "\n",
    "# setting the parameters for a novelty-seeking agent\n",
    "pN = parameters(A); η = param2η(pN);\n",
    "η = merge(η, (;κ = 1.))\n",
    "η = merge(η, (;β_MFe_1 = 0., β_MFe_2_G0 = 0., β_MFe_2_G1 = 0., β_MFe_2_G2 = 0.))\n",
    "η = merge(η, (;β_MFi_1 = 0., β_MFi_2_G0 = 0., β_MFi_2_G1 = 0., β_MFi_2_G2 = 0.))\n",
    "η = merge(η, (;r_1 = 2., r_2 = 10.))\n",
    "η = merge(η, (;wN = 5., wI = 0., wS = 0.));\n",
    "pN = parameters(A,η);\n",
    "# setting the parameters for a surprise-seeking agent\n",
    "η = merge(η, (;wN = 0., wI = 0., wS = 5.));\n",
    "pS = parameters(A,η);\n",
    "# setting the parameters for a information-gain-seeking agent\n",
    "η = merge(η, (;wN = 0., wI = 5., wS = 0.));\n",
    "pI = parameters(A,η);\n",
    "# setting the parameters for an agent with no intrinsic reward\n",
    "η = merge(η, (;wN = 0., wI = 0., wS = 0.));\n",
    "pR = parameters(A,η);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to convert participants' data to sequence of `Str_Agent_State`, using `Str_Input2Agents`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_ns = Str_Input2Agents.(Data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at participant 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sub = 20\n",
    "draw = Data[n_sub]\n",
    "d = Data_ns[n_sub];\n",
    "\n",
    "lpsN, APolN = logp_pass_agent(d, A, pN); \n",
    "lpsS, APolS = logp_pass_agent(d, A, pS); \n",
    "lpsI, APolI = logp_pass_agent(d, A, pI); \n",
    "lpsR, APolR = logp_pass_agent(d, A, pR); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `logp_pass_agent` takes as input the data of a praticipant `d`, an agent type `A`, and a set of parameters, e.g., `pN`. Then it returns:\n",
    "1. `lps[i]`: the log-likelihood of data for episode `i`\n",
    "2. `APol[i]`: the sequece of `Str_Agent_Policy` during episode `i`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can, for example, compare the log-likelihood of the different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEpi = 5\n",
    "fig = figure(figsize=(6,6));\n",
    "ax = subplot(1,1,1)\n",
    "ax.bar((0:5:(5*(NEpi-1))) .+ 0, lpsN .- lpsR)\n",
    "ax.bar((0:5:(5*(NEpi-1))) .+ 1, lpsI .- lpsR)\n",
    "ax.bar((0:5:(5*(NEpi-1))) .+ 2, lpsS .- lpsR)\n",
    "\n",
    "ax.legend([\"novelty-seeking\", \"inf-gain-seeking\", \"surprise-seeking\"])\n",
    "ax.set_xticks((0:5:(5*(NEpi-1))) .+ 1); ax.set_xticklabels(1:NEpi)\n",
    "ax.plot([0,5*5] .- 1.5,[0,0],\"--k\"); ax.set_xlim([0,5*5] .- 1.5)\n",
    "\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"log P( Data | Model ) -  log P( Data | nIR )\")\n",
    "\n",
    "tight_layout()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can look at the sequence Q-values of specific actions.\n",
    "For example, let's look at the Q-values of different actions in state 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding of state 1 as an image\n",
    "state = 4\n",
    "St = [0,state - 1]; \n",
    "\n",
    "# for intrinsic and extrinsic Q-values\n",
    "Qis = Vector{Vector{Float64}}([])\n",
    "Qes = Vector{Vector{Float64}}([])\n",
    "As  = []\n",
    "\n",
    "# looking at episode 1\n",
    "n_epi = 1\n",
    "\n",
    "# looping over time steps\n",
    "T = length(APolN[n_epi])\n",
    "for t = 1:T\n",
    "    # checking if the agent has observed St yet\n",
    "    if St ∈ d[n_epi].AStates[t].State_Set\n",
    "        sid = findmax([St == s for s = d[n_epi].AStates[t].State_Set])[2]\n",
    "        push!(Qis, APolN[n_epi][t].Q_MBi[sid,:])\n",
    "        push!(Qes, APolN[n_epi][t].Q_MBe[sid,:])\n",
    "\n",
    "        if d[n_epi].AStates[t].S_t == St\n",
    "            push!(As, [t,d[n_epi].AStates[t].A_t])\n",
    "        end\n",
    "    else\n",
    "        push!(Qis, [1,1,1] .* NaN)\n",
    "        push!(Qes, [1,1,1] .* NaN)\n",
    "    end\n",
    "end\n",
    "As = hcat(As...); Qis = hcat(Qis...); Qes = hcat(Qes...);\n",
    "\n",
    "# plotting\n",
    "fig = figure(figsize=(6,10));\n",
    "ax = subplot(4,1,1)\n",
    "ax.plot(1:T, draw.states[n_epi],\"-o\",color=\"k\")\n",
    "ax.plot((1:T)[draw.states[n_epi] .== state],\n",
    "        draw.states[n_epi][draw.states[n_epi] .== state],\"o\",color=\"r\")\n",
    "ax.set_xlim([0,T+1])\n",
    "ax.set_xlabel(\"trials within episode \" * string(n_epi))\n",
    "ax.set_ylabel(\"state\")\n",
    "ax.legend([\"all states\",\"in state \" * string(state)])\n",
    "\n",
    "ax = subplot(4,1,2)\n",
    "ax.plot(1:T, draw.actions[n_epi],\"-o\",color=\"k\")\n",
    "ax.plot((1:T)[draw.states[n_epi] .== state], \n",
    "        draw.actions[n_epi][draw.states[n_epi] .== state],\"o\",color=\"r\")\n",
    "ax.set_xlim([0,T+1])\n",
    "ax.set_xlabel(\"trials within episode \" * string(n_epi))\n",
    "ax.set_ylabel(\"action\")\n",
    "ax.legend([\"all states\",\"in state \" * string(state)])\n",
    "\n",
    "ax = subplot(4,1,3)\n",
    "for i = 1:3\n",
    "    ax.plot(1:T,Qis[i,:] .- Qis[1,:])\n",
    "end\n",
    "ax.set_xlim([0,T+1])\n",
    "ax.set_xlabel(\"trials within episode \" * string(n_epi))\n",
    "ax.set_ylabel(\"Q_i(a,s=\" * string(state) * \") - Q_i(a1,s=\" * string(state) * \")\")\n",
    "ax.legend([\"a1\",\"a2\",\"a3\"])\n",
    "\n",
    "\n",
    "ax = subplot(4,1,4)\n",
    "for i = 1:3\n",
    "    ax.plot(1:T,Qes[i,:] .- Qes[1,:])\n",
    "end\n",
    "ax.set_xlim([0,T+1])\n",
    "ax.set_xlabel(\"trials within episode \" * string(n_epi))\n",
    "ax.set_ylabel(\"Q_e(a,s=\" * string(state) * \") - Q_e(a1,s=\" * string(state) * \")\")\n",
    "ax.legend([\"a1\",\"a2\",\"a3\"])\n",
    "\n",
    "\n",
    "tight_layout()\n",
    "display(fig)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
